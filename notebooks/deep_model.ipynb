{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyObhyvCCAfZ9ZQPA+8ZF+Mg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssttefann/EmotionClassification/blob/master/notebooks/deep_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0NC2t-gbt_s",
        "colab_type": "text"
      },
      "source": [
        "# **Emotion Classificaton in Text - Deep Learning Approach**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MXEryxobzA8",
        "colab_type": "code",
        "outputId": "a762e253-2afe-4bcf-9da8-3090f2d55791",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# Tensorflow\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  !pip install -q tf-nightly\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "# General\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot') \n",
        "# plt.style.use('dark_background')\n",
        "\n",
        "try:\n",
        "  import pyprind\n",
        "except Exception:\n",
        "  !pip install pyprind\n",
        "  import pyprind\n",
        "\n",
        "# Data preprocessing\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ikaMJvQ1Y9A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acd3NDN3cDFi",
        "colab_type": "text"
      },
      "source": [
        "## Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZ2tWUNzcEDe",
        "colab_type": "code",
        "outputId": "65c7ccc1-3cb7-406e-cc4b-c1d3fe5c7acf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# set the seed for entire process\n",
        "# so that results can be reproducible\n",
        "np.random.seed(123)\n",
        "\n",
        "dataset = pd.read_csv(\"/content/drive/My Drive/emotion.data\")\n",
        "dataset.drop(dataset.columns[0], axis='columns', inplace=True)    # drop id column since it won't be used \n",
        "dataset = dataset.reindex(np.random.permutation(dataset.index))   # shuffle data \n",
        "dataset.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>emotions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>98301</th>\n",
              "      <td>i am feeling melancholy and have finally pinpo...</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41147</th>\n",
              "      <td>i miss having someone to talk to who i have th...</td>\n",
              "      <td>joy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151024</th>\n",
              "      <td>i just feel like i get blamed for everything</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202104</th>\n",
              "      <td>i also apologize for mentioning about him in m...</td>\n",
              "      <td>anger</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73602</th>\n",
              "      <td>im finding is the difference in having a life ...</td>\n",
              "      <td>joy</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     text emotions\n",
              "98301   i am feeling melancholy and have finally pinpo...  sadness\n",
              "41147   i miss having someone to talk to who i have th...      joy\n",
              "151024       i just feel like i get blamed for everything  sadness\n",
              "202104  i also apologize for mentioning about him in m...    anger\n",
              "73602   im finding is the difference in having a life ...      joy"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIT8NFuUcIn_",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQEc-EQecJo_",
        "colab_type": "code",
        "outputId": "a30235c0-9bf2-43a6-e8d4-00a318d91048",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "try:\n",
        "  stop = stopwords.words('english')\n",
        "except LookupError:\n",
        "  import nltk\n",
        "  nltk.download('stopwords')\n",
        "  stop = stopwords.words('english')\n",
        "\n",
        "stop.extend(['img', 'src', 'href'])     # some of these appear in given dataset\n",
        "print(stop[:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stGdoVPIcNDb",
        "colab_type": "text"
      },
      "source": [
        "### Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tKeTx9YcN-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def tokenizer(text):\n",
        "    ''' Removes stop words and special characters,\n",
        "        and returns list of all words that are left\n",
        "\n",
        "        Parameters\n",
        "        -----------\n",
        "        text : string \n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        tokenized : list {string}\n",
        "    '''\n",
        "\n",
        "    text = re.sub('http.*', '', text)                               # delete link references\n",
        "    text = re.sub('[\\W]+', ' ', text.lower()).replace('-', '')      # delete non-word characters [^a-zA-Z0-9]\n",
        "    tokenized = [w for w in text.split() if w not in stop]          # delete stop words ( I, me , a, the)\n",
        "    return tokenized\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0O6PryacTLP",
        "colab_type": "text"
      },
      "source": [
        "### Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNl3RKrecUqw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "porter = PorterStemmer() \n",
        "def tokenizer_porter(text):\n",
        "  return [porter.stem(word) for word in tokenizer(text)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inDELbsjconI",
        "colab_type": "text"
      },
      "source": [
        "## Creating Train and Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvwvSgPIcqTQ",
        "colab_type": "code",
        "outputId": "8e574260-33ce-4aa1-e2a6-583bacc463fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "label2id = {\"joy\" : 0, \"sadness\" : 1, \"anger\" : 2, \"fear\" : 3, \"love\" : 4, \"surprise\" : 5}\n",
        "id2label = {0 : \"joy\", 1 : \"sadness\", 2 : \"anger\", 3 : \"fear\", 4 : \"love\", 5 : \"surprise\"}\n",
        "\n",
        "label2col = {\"joy\":\"yellow\", \"sadness\":\"blue\", \"anger\":\"red\", \"fear\":\"grey\", \"love\":\"pink\", \"surprise\":\"orange\"}\n",
        "\n",
        "#label2id = { label : idx for idx, label in enumerate(set(dataset['emotions'].values))}\n",
        "#id2label = { id : label for label, id in label2id.items()}\n",
        "\n",
        "print(label2id)\n",
        "print(id2label)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'joy': 0, 'sadness': 1, 'anger': 2, 'fear': 3, 'love': 4, 'surprise': 5}\n",
            "{0: 'joy', 1: 'sadness', 2: 'anger', 3: 'fear', 4: 'love', 5: 'surprise'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRfrqeW6cru1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset['text'].values, \n",
        "                                                    dataset['emotions'].values, \n",
        "                                                    test_size=0.1, \n",
        "                                                    random_state=123)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QElcfdEZ1QK0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = [ label2id[val] for val in y_train]\n",
        "y_test =  [ label2id[val] for val in y_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFUsnhYDcvim",
        "colab_type": "text"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dnct11bcw_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = 10000\n",
        "embedding_dim = 16\n",
        "max_length = 100\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "training_size = 20000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzy03I1u2tgJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "training_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "testing_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIf3KvJL-b6G",
        "colab_type": "code",
        "outputId": "9d33512c-70b6-4ceb-e182-113b708cb1e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train = keras.utils.to_categorical(y_train, num_classes=len(label2id), dtype='float32')\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes=len(label2id), dtype='float32')\n",
        "\n",
        "y_test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(41681, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGzhhKxg92Fe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim =  16\n",
        "max_words = max_length\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(6, activation='softmax')\n",
        "])\n",
        "model.compile(loss=\"categorical_crossentropy\",\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tir9LG9s3HRA",
        "colab_type": "code",
        "outputId": "ad8a1607-d9ce-4c01-9790-711fb9fabb01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 100)          1000000   \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 128)               84480     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 6)                 390       \n",
            "=================================================================\n",
            "Total params: 1,093,126\n",
            "Trainable params: 1,093,126\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IugV_LoN3I9U",
        "colab_type": "code",
        "outputId": "9e61a79e-9db9-4050-954c-6f43d8bedaae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "num_epochs = 30\n",
        "history = model.fit(training_padded, y_train, epochs=num_epochs, validation_data=(testing_padded, y_test))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "11723/11723 [==============================] - 966s 82ms/step - loss: 0.8278 - accuracy: 0.6912 - val_loss: 0.3881 - val_accuracy: 0.8598\n",
            "Epoch 2/30\n",
            "11723/11723 [==============================] - 965s 82ms/step - loss: 0.3077 - accuracy: 0.8868 - val_loss: 0.2667 - val_accuracy: 0.8956\n",
            "Epoch 3/30\n",
            "11723/11723 [==============================] - 971s 83ms/step - loss: 0.2396 - accuracy: 0.9038 - val_loss: 0.2314 - val_accuracy: 0.9024\n",
            "Epoch 4/30\n",
            "11723/11723 [==============================] - 963s 82ms/step - loss: 0.2064 - accuracy: 0.9117 - val_loss: 0.2071 - val_accuracy: 0.9067\n",
            "Epoch 5/30\n",
            "10573/11723 [==========================>...] - ETA: 1:33 - loss: 0.1812 - accuracy: 0.9178Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULXCC7SZ3RLE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "db8f2ae2-302b-4399-84ea-cb4eb1b89a9f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "  \n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-1feca8b3a752>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mplot_graphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mplot_graphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    }
  ]
}